Applied Assumption Tree
Drag to move · Double-click to edit · Hover × to delete
AI Causes Catastrophic Harm
AND — ALL THREE BRANCHES REQUIRED

Optimization
AND — ALL 4 CONDITIONS REQUIRED
The 4 I's determine optimization power O and capability function g(o); O applies to individual systems or aggregate ecosystem
Cognitive Competence (Intellgence)
AND
 

AI matches or exceeds human performance in a decisive domain.




Goal-Directed Behavior (Interest)
AND


AI actions exhibit stable patterns consistent with optimizing for identifiable outcomes.




Real-World Influence (Influence)
OR


Digital system access

Robotic / physical actuators

Social / persuasive influence


Resource Acquisition (Inputs)
OR


Compute / energy access

Financial resources

Human organizational leverage

Misalignment
OR — ANY PATHWAY SUFFICIENT
O directed toward Doom, Determines misalignment probability pO(o)
Autonomous Misalignment (AI Takeover)
OR
High pO(o) at high O — system's own objectives diverge

Objectives differ from designers' intent in a misaligned way

Alignment degrades over time or context

System conceals its true objectives
Human Misuse (AI as Tool)
OR
pO(o) depends on actor access and O levels

Intentional harmful use

Unsafe deployment (insufficient safeguards)

Harmful repurposing of capabilities
Structural / Emergent (Universe 25)
OR
pO(o) increases gradually as aggregate O increases

Power concentration

Institutional erosion

Lock-in / irreversibility

Multi-agent interaction failures

Failed Correction
OR — ANY FAILURE SUFFICIENT
Environment E determines what level of O produces D
Detection Failure
OR
Absent monitoring → E becomes permissive → g(o) and pO(o) unconstrained

System behavior is opaque

Active concealment

Normalization / gradual harm
Willingness Failure
OR
Known risk but no action → E remains permissive → g(o) and pO(o) increase

Economic lock-in / dependency

Political / regulatory capture

Optimism bias / risk tolerance
Capability Failure
OR
E cannot reduce pO(o) even when misalignment is detected

Technical intractability

Response speed mismatch

Infrastructure integration lock-in

Irreversible harm already realized
Coordination Failure
OR
Fragmented governance → no binding E → g(o) and pO(o) unconstrained

Geopolitical competition

Jurisdictional arbitrage

Governance speed mismatch
Risk Amplifiers
Modify environment E, shifting multiple conditions simultaneously
Corporate incentive structures rewarding deployment speed over safety · Information asymmetry and secrecy between actors · Recursive self-improvement potential · Broad voluntary adoption of AI systems · Deployment pressure and race dynamics
AND
All sub-conditions required
OR
Any sub-condition sufficient Double-click to edit · Hover × to delete
Reset All
